{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big data case study - NYC Parking Tickets: An Exploratory Analysis                                                                    \n",
    " Members:                                                                                                \n",
    " 1.Bhanu Pratap                                                                                            \n",
    " 2.Shadab Hussain                                                                                     \n",
    " 3.Md. Liyakat                                                                                           \n",
    " 4.Nidhi Tripathi\n",
    " \n",
    " ### Brief on subject: \n",
    "The NYC Department of Finance collects data on every parking ticket issued in NYC (~10M per year!). This data is made publicly \n",
    "available to aid in ticket resolution and to guide policymakers. New York City is a thriving metropolis. Just like most other \n",
    "metros that size, one of the biggest problems its citizens face, is parking. The classic combination of a huge number of cars,\n",
    "and a cramped geography is the exact recipe that leads to a huge number of parking tickets. In an attempt to scientifically analyse\n",
    "this phenomenon, the NYC Police Department has collected data for parking tickets. Out of these, the data files from 2014 to 2017\n",
    "are publicly available on Kaggle. We will try and perform some exploratory analysis on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Initialising spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class pyspark.sql.SparkSession, The entry point to programming Spark with the Dataset and DataFrame API.\n",
    "#A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and \n",
    "# read parquet files.To create a SparkSession, use the following builder pattern:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark DataFrame and Sql\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data if spark dataframe from the data stored in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(header='false') \\\n",
    "     .load('/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispalying first 5 rows of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|           _c0|     _c1|               _c2|       _c3|           _c4|              _c5|         _c6|               _c7|            _c8|           _c9|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFame will have columns, and we use a schema to define them.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|           _c0|     _c1|               _c2|       _c3|           _c4|              _c5|         _c6|               _c7|            _c8|           _c9|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema defines the column names and types of a DataFrame. We can either let a data source define the schema\n",
    "# (called schema-on-read) or we can define it explicitly ourselves.\n",
    "#A schema is a StructType made up of a number of fields, StructFields, that have a name, type, a Boolean flag which\n",
    "#specifies whether that column can contain missing or null values, and, finally, users can optionally specify associated\n",
    "#metadata with that column.\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"Summons_Number\", StringType(), True),\n",
    "  StructField(\"Plate_ID\", StringType(), True),\n",
    "  StructField(\"Registration_State\", StringType(), True),\n",
    "  StructField(\"Issue_Date\", StringType(), True),\n",
    "  StructField(\"Violation_Code\", StringType(), True),    \n",
    "  StructField(\"Vehicle_Body_Type\", StringType(), True),    \n",
    "  StructField(\"Vehicle_Make\", StringType(), True),    \n",
    "  StructField(\"Violation_Precinct\", StringType(), True),\n",
    "  StructField(\"Issuer_Precinct\", StringType(), True),    \n",
    "  StructField(\"Violation_Time\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.format('csv').schema(myManualSchema).load('/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Registration_State|Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the 1st Row as it is redundant and contains columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Registration_State|Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.filter(df['Summons_Number']!='Summons Number')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Issue_Date to Issued data in pySpark Date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "df = df.withColumn(\"Issued_date\",df['Issue_Date'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: string (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Issue_Date: string (nullable = true)\n",
      " |-- Violation_Code: string (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: string (nullable = true)\n",
      " |-- Issuer_Precinct: string (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      " |-- Issued_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|Issued_date|Issue_Date|\n",
      "+-----------+----------+\n",
      "| 2016-07-10|2016-07-10|\n",
      "| 2016-07-08|2016-07-08|\n",
      "| 2016-08-23|2016-08-23|\n",
      "| 2017-06-14|2017-06-14|\n",
      "| 2016-11-21|2016-11-21|\n",
      "+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Issued_date','Issue_Date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Issue_Date Columns as we don't need it anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Issue_Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting data only from Year 2017 in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+\n",
      "|Summons_Number|Plate_ID|Registration_State|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|Issued_date|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+\n",
      "|    8478629828| 66623ME|                NY|            47|             REFG|       MITSU|                14|             14|         1120A| 2017-06-14|\n",
      "|    5096917368| FZD8593|                NY|             7|             SUBN|       ME/BE|                 0|              0|         0852P| 2017-06-13|\n",
      "|    1407740258| 2513JMG|                NY|            78|             DELV|       FRUEH|               106|            106|         0015A| 2017-01-11|\n",
      "|    1413656420|T672371C|                NY|            40|             TAXI|       TOYOT|                73|             73|         0525A| 2017-02-04|\n",
      "|    8480309064| 51771JW|                NY|            64|              VAN|       INTER|                17|             17|         0256P| 2017-01-26|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df = df.filter((df[\"Issued_date\"]>lit(\"2016-12-31\")) & (df[\"Issued_date\"]<lit(\"2017-12-31\")) )\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conveting Custom Violation Time column into correct time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12:20:00'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_formatting(time):\n",
    "    if time[-1]=='P' and time[:2]!=\"12\":\n",
    "            return str(int(time[:2])+12)+\":\"+time[2:4]+\":\"+\"00\"\n",
    "    else:\n",
    "        return time[:2]+\":\"+time[2:4]+\":\"+\"00\"\n",
    "time_formatting(\"1220P\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType, StringType\n",
    "\n",
    "time_udf = udf(lambda time: time_formatting(time), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Time\", time_udf(df.Violation_Time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|Violation_Time|    Time|\n",
      "+--------------+--------+\n",
      "|         1120A|11:20:00|\n",
      "|         0852P|20:52:00|\n",
      "|         0015A|00:15:00|\n",
      "|         0525A|05:25:00|\n",
      "|         0256P|14:56:00|\n",
      "+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Violation_Time', 'Time').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons_Number: string (nullable = true)\n",
      " |-- Plate_ID: string (nullable = true)\n",
      " |-- Registration_State: string (nullable = true)\n",
      " |-- Violation_Code: string (nullable = true)\n",
      " |-- Vehicle_Body_Type: string (nullable = true)\n",
      " |-- Vehicle_Make: string (nullable = true)\n",
      " |-- Violation_Precinct: string (nullable = true)\n",
      " |-- Issuer_Precinct: string (nullable = true)\n",
      " |-- Violation_Time: string (nullable = true)\n",
      " |-- Issued_date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, to_date\n",
    "# df = df.withColumn(\"Violation_Time\", to_date(col(\"Time\"), \"HH:mm:ss\"))\n",
    "# df.select(\"Violation_Time\",\"Issued_date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voilation Time is in Time Format Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+--------+\n",
      "|Summons_Number|Plate_ID|Registration_State|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|Issued_date|    Time|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+--------+\n",
      "|    8478629828| 66623ME|                NY|            47|             REFG|       MITSU|                14|             14|         1120A| 2017-06-14|11:20:00|\n",
      "|    5096917368| FZD8593|                NY|             7|             SUBN|       ME/BE|                 0|              0|         0852P| 2017-06-13|20:52:00|\n",
      "|    1407740258| 2513JMG|                NY|            78|             DELV|       FRUEH|               106|            106|         0015A| 2017-01-11|00:15:00|\n",
      "|    1413656420|T672371C|                NY|            40|             TAXI|       TOYOT|                73|             73|         0525A| 2017-02-04|05:25:00|\n",
      "|    8480309064| 51771JW|                NY|            64|              VAN|       INTER|                17|             17|         0256P| 2017-01-26|14:56:00|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDMS temp Table from DataFrame for Executing SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "# after registering temp table you can run your sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+--------+\n",
      "|Summons_Number|Plate_ID|Registration_State|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|Issued_date|    Time|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+--------+\n",
      "|    8478629828| 66623ME|                NY|            47|             REFG|       MITSU|                14|             14|         1120A| 2017-06-14|11:20:00|\n",
      "|    5096917368| FZD8593|                NY|             7|             SUBN|       ME/BE|                 0|              0|         0852P| 2017-06-13|20:52:00|\n",
      "|    1407740258| 2513JMG|                NY|            78|             DELV|       FRUEH|               106|            106|         0015A| 2017-01-11|00:15:00|\n",
      "|    1413656420|T672371C|                NY|            40|             TAXI|       TOYOT|                73|             73|         0525A| 2017-02-04|05:25:00|\n",
      "|    8480309064| 51771JW|                NY|            64|              VAN|       INTER|                17|             17|         0256P| 2017-01-26|14:56:00|\n",
      "+--------------+--------+------------------+--------------+-----------------+------------+------------------+---------------+--------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('Select * from dfTable limit 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1). Find the total number of tickets for the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Ticket_Count|\n",
      "+------------+\n",
      "|     5431909|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(Summons_Number) as Ticket_Count FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are total `5431909` tickets for year 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Find out the number of unique states from where the cars that got parking tickets came."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SQL query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Distinct_State_Count|\n",
      "+--------------------+\n",
      "|                  65|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(Distinct(Registration_State)) as Distinct_State_Count FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using PySpark inbuilt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Distinct_State_Count|\n",
      "+--------------------+\n",
      "|                  65|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"Registration_State\").alias('Distinct_State_Count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|State_Count|Registration_State|\n",
      "+-----------+------------------+\n",
      "|          8|                FO|\n",
      "|          9|                SK|\n",
      "|         17|                MB|\n",
      "|         38|                PR|\n",
      "|         54|                BC|\n",
      "|         57|                NB|\n",
      "|         61|                PE|\n",
      "|         79|                AB|\n",
      "|        156|                HI|\n",
      "|        188|                WY|\n",
      "|        254|                ND|\n",
      "|        298|                AK|\n",
      "|        322|                NS|\n",
      "|        348|                GV|\n",
      "|        505|                MT|\n",
      "|        561|                UT|\n",
      "|        704|                NE|\n",
      "|        706|                KS|\n",
      "|        725|                NV|\n",
      "|        763|                ID|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(*) as State_Count , Registration_State  FROM dfTable group by Registration_State \\\n",
    "          ORDER by State_Count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering State with Max Count of tickets for Replacement with 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|State_Count|Registration_State|\n",
      "+-----------+------------------+\n",
      "|    4273944|                NY|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(*) as State_Count , Registration_State  FROM dfTable group by Registration_State \\\n",
    "          ORDER by State_Count Desc limit 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing the `99` in registration state with the `NY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "targetDf = df.withColumn(\"Registration_State\", \\\n",
    "              when(df[\"Registration_State\"] == '99', 'NY').otherwise(df['Registration_State']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|State_Count|Registration_State|\n",
      "+-----------+------------------+\n",
      "|    4289999|                NY|\n",
      "|     475824|                NJ|\n",
      "|     140285|                PA|\n",
      "|      70403|                CT|\n",
      "|      69468|                FL|\n",
      "|      45525|                IN|\n",
      "|      38941|                MA|\n",
      "|      34367|                VA|\n",
      "|      30213|                MD|\n",
      "|      27152|                NC|\n",
      "|      18827|                TX|\n",
      "|      18666|                IL|\n",
      "|      17537|                GA|\n",
      "|      12379|                AZ|\n",
      "|      12281|                OH|\n",
      "|      12153|                CA|\n",
      "|      10806|                ME|\n",
      "|      10395|                SC|\n",
      "|      10083|                MN|\n",
      "|       9088|                OK|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targetDf.createOrReplaceTempView(\"dfTable\")\n",
    "spark.sql('SELECT count(*) as State_Count , Registration_State  FROM dfTable group by Registration_State \\\n",
    "          ORDER by State_Count Desc').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see 99 is not present in the list anymore, we can evaluate count of Distinct State with this updated Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Distinct_State_Count|\n",
      "+--------------------+\n",
      "|                  64|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targetDf.select(countDistinct(\"Registration_State\").alias('Distinct_State_Count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***So there are 64 unique state***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1). How often does each violation code occur? Display the frequency of the top five violation codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|Distinct_Violation_Code_Count|\n",
      "+-----------------------------+\n",
      "|                          100|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(distinct(Violation_Code)) as Distinct_Violation_Code_Count FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation_Code|Violation_Code_Count|\n",
      "+--------------+--------------------+\n",
      "|            21|              768085|\n",
      "|            36|              662765|\n",
      "|            38|              542079|\n",
      "|            14|              476664|\n",
      "|            20|              319644|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code, count(*) as Violation_Code_Count   FROM dfTable group by Violation_Code \\\n",
    "          ORDER by Violation_Code_Count Desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 5 Violation_Code with frequency are following:-<br>\n",
    "1. `21`  occurance `768085`\n",
    "2. `36`  occurance `662765`\n",
    "3. `38`  occurance `542079`\n",
    "4. `14`  occurance `476664`\n",
    "5. `20`  occurance `319644`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2). How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'? (Hint: Find the top 5 for both)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis for how often does each 'vehicle body type' get parking ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|Distinct_Vehicle_Body_Type_Count|\n",
      "+--------------------------------+\n",
      "|                            1165|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(distinct(Vehicle_Body_Type)) as Distinct_Vehicle_Body_Type_Count FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+\n",
      "|Vehicle_Body_Type|Vehicle_Body_Type_Count|\n",
      "+-----------------+-----------------------+\n",
      "|             SUBN|                1883953|\n",
      "|             4DSD|                1547312|\n",
      "|              VAN|                 724027|\n",
      "|             DELV|                 358984|\n",
      "|              SDN|                 194191|\n",
      "+-----------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Vehicle_Body_Type, count(*) as Vehicle_Body_Type_Count   FROM dfTable group by Vehicle_Body_Type \\\n",
    "          ORDER by Vehicle_Body_Type_Count Desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 5 vehicle body type which get parking tickets are followings:-<br>\n",
    "1. `SUBN` gets parking ticktes `1883369` times\n",
    "2. `4DSD` gets parking ticktes `1547312` times\n",
    "3. `VAN`  gets parking ticktes `723783`  times\n",
    "4. `DELV` gets parking ticktes `358779`  times\n",
    "5. `SDN`  gets parking ticktes `193453` times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis for how often does each 'vehicle Make' get parking ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|Distinct_Vehicle_Make_Count|\n",
      "+---------------------------+\n",
      "|                       3179|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT count(distinct(Vehicle_Make)) as Distinct_Vehicle_Make_Count FROM dfTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|Vehicle_Make|Vehicle_Make_Count|\n",
      "+------------+------------------+\n",
      "|        FORD|            636843|\n",
      "|       TOYOT|            605288|\n",
      "|       HONDA|            538884|\n",
      "|       NISSA|            462017|\n",
      "|       CHEVR|            356031|\n",
      "+------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Vehicle_Make, count(*) as Vehicle_Make_Count   FROM dfTable group by Vehicle_Make \\\n",
    "          ORDER by Vehicle_Make_Count Desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 5 vehicle make which get parking tickets are followings:-<br>\n",
    "1. `FORD`  gets parking ticktes `636843` times\n",
    "2. `TOYOT` gets parking ticktes `605288` times\n",
    "3. `HONDA` gets parking ticktes `538884` times\n",
    "4. `NISSA` gets parking ticktes `462017` times\n",
    "5. `CHEVR` gets parking ticktes `356031` times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3). A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following:\n",
    "- **1). 'Violation Precinct' (This is the precinct of the zone where the violation occurred). <br>  Using this, can you draw any insights for parking violations in any specific areas of the city?<br>**\n",
    "- **2). 'Issuer Precinct' (This is the precinct that issued the ticket.)**\n",
    "\n",
    "Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts. **\n",
    "(Hint: Print the top six entries after sorting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------------+\n",
      "|Violation_Precinct|Violation_Precinct_Count|\n",
      "+------------------+------------------------+\n",
      "|                 0|                  925595|\n",
      "|                19|                  274444|\n",
      "|                14|                  203553|\n",
      "|                 1|                  174702|\n",
      "|                18|                  169131|\n",
      "|               114|                  147444|\n",
      "+------------------+------------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Precinct, count(*) as Violation_Precinct_Count   FROM dfTable  group by Violation_Precinct \\\n",
    "          ORDER by Violation_Precinct_Count Desc').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 5 Violation Precinct are `19`, `14`, `1`, `18`, `114`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|Issuer_Precinct|Issuer_Precinct_Count|\n",
      "+---------------+---------------------+\n",
      "|              0|              1078404|\n",
      "|             19|               266961|\n",
      "|             14|               200495|\n",
      "|              1|               168740|\n",
      "|             18|               162994|\n",
      "|            114|               144054|\n",
      "+---------------+---------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Issuer_Precinct, count(*) as Issuer_Precinct_Count   FROM dfTable group by Issuer_Precinct \\\n",
    "          ORDER by Issuer_Precinct_Count Desc').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The top 5 Issuer Precinct are `19`, `14`, `1`, `18`, `114`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4). Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|Issuer_Precinct|Issuer_Precinct_Count|\n",
      "+---------------+---------------------+\n",
      "|              0|              1078404|\n",
      "|             19|               266961|\n",
      "|             14|               200495|\n",
      "|              1|               168740|\n",
      "+---------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Issuer_Precinct, count(*) as Issuer_Precinct_Count FROM dfTable group by Issuer_Precinct \\\n",
    "          ORDER by Issuer_Precinct_Count Desc limit 4').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|Issuer_Precinct|Violation_Code|Violation_Code_Count|\n",
      "+---------------+--------------+--------------------+\n",
      "|             19|            46|               48445|\n",
      "|             14|            14|               45036|\n",
      "|              1|            14|               38354|\n",
      "|             19|            38|               36386|\n",
      "|             19|            37|               36056|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Issuer_Precinct,Violation_Code, count(*) as Violation_Code_Count  FROM dfTable \\\n",
    "            where Issuer_Precinct in ( \"19\", \"14\", \"1\") \\\n",
    "            group by Violation_Code,Issuer_Precinct \\\n",
    "            ORDER by Violation_Code_Count Desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precinct 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|Issuer_Precinct|Violation_Code|Violation_Code_Count|\n",
      "+---------------+--------------+--------------------+\n",
      "|             19|            46|               48445|\n",
      "|             19|            38|               36386|\n",
      "|             19|            37|               36056|\n",
      "|             19|            14|               29797|\n",
      "|             19|            21|               28415|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Issuer_Precinct,Violation_Code, count(*) as Violation_Code_Count  FROM dfTable \\\n",
    "            where Issuer_Precinct = \"19\" \\\n",
    "            group by Violation_Code,Issuer_Precinct \\\n",
    "            ORDER by Violation_Code_Count Desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In `Precinct 19`, top 5 Violation_Code are `46`, `38`, `37`, `14`, `21`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precinct 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|Issuer_Precinct|Violation_Code|Violation_Code_Count|\n",
      "+---------------+--------------+--------------------+\n",
      "|             14|            14|               45036|\n",
      "|             14|            69|               30464|\n",
      "|             14|            31|               22555|\n",
      "|             14|            47|               18364|\n",
      "|             14|            42|               10027|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Issuer_Precinct,Violation_Code, count(*) as Violation_Code_Count  FROM dfTable \\\n",
    "            where Issuer_Precinct = \"14\" \\\n",
    "            group by Violation_Code,Issuer_Precinct \\\n",
    "            ORDER by Violation_Code_Count Desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Precinct 14` top 5 Violation_Code are `14`,`69`,`31`,`47`, `42`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precinct 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+\n",
      "|Issuer_Precinct|Violation_Code|Violation_Code_Count|\n",
      "+---------------+--------------+--------------------+\n",
      "|              1|            14|               38354|\n",
      "|              1|            16|               19081|\n",
      "|              1|            20|               15408|\n",
      "|              1|            46|               12745|\n",
      "|              1|            38|                8535|\n",
      "+---------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Issuer_Precinct,Violation_Code, count(*) as Violation_Code_Count  FROM dfTable \\\n",
    "            where Issuer_Precinct = \"1\" \\\n",
    "            group by Violation_Code,Issuer_Precinct \\\n",
    "            ORDER by Violation_Code_Count Desc').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Precinct 1` top 5 Violation_Code are `14`,`16`,`20`,`46`, `38`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find out the properties of parking violations across different times of the day:\n",
    "- Find a way to deal with missing values, if any.\n",
    "(Hint: Check for the null values using 'isNull' under the SQL. Also, to remove the null values, check the 'dropna' command in the API documentation.)\n",
    "\n",
    "- The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups.\n",
    "\n",
    "- Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations.\n",
    "(Hint: Use the CASE-WHEN in SQL view to segregate into bins. To find the most commonly occurring violations, you can use an approach similar to the one mentioned in the hint for question 4.)\n",
    "\n",
    "- Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Time|\n",
      "+--------+\n",
      "|11:20:00|\n",
      "|20:52:00|\n",
      "|00:15:00|\n",
      "|05:25:00|\n",
      "|14:56:00|\n",
      "|12:32:00|\n",
      "|10:34:00|\n",
      "|10:21:00|\n",
      "|07:21:00|\n",
      "|09:40:00|\n",
      "|12:23:00|\n",
      "|10:28:00|\n",
      "|01:48:00|\n",
      "|12:06:00|\n",
      "|13:41:00|\n",
      "|08:22:00|\n",
      "|08:20:00|\n",
      "|10:43:00|\n",
      "|14:04:00|\n",
      "|08:53:00|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_bins(time):\n",
    "    if time[:2] in [\"00\", \"01\", \"02\" , \"03\"]:\n",
    "        return 'Slot_1'\n",
    "    elif time[:2] in [\"04\", \"05\", \"06\" , \"07\"]:\n",
    "        return 'Slot_2'\n",
    "    elif time[:2] in [\"08\", \"09\", \"10\" , \"11\"]:\n",
    "        return 'Slot_3'\n",
    "    elif time[:2] in [\"12\", \"13\", \"14\" , \"15\"]:\n",
    "        return 'Slot_4'\n",
    "    elif time[:2] in [\"16\", \"17\", \"18\" , \"19\"]:\n",
    "        return 'Slot_5'\n",
    "    elif time[:2] in [\"20\", \"21\", \"22\" , \"23\"]:\n",
    "        return 'Slot_6'\n",
    "    else:\n",
    "        return 'None'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "timebin_udf = udf(lambda time : time_bins(time), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Time_Bin\" , timebin_udf(df.Time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    Time|Time_Bin|\n",
      "+--------+--------+\n",
      "|11:20:00|  Slot_3|\n",
      "|20:52:00|  Slot_6|\n",
      "|00:15:00|  Slot_1|\n",
      "|05:25:00|  Slot_2|\n",
      "|14:56:00|  Slot_4|\n",
      "+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Time','Time_Bin').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df['Time_Bin'] != 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('binTab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying top 3 Violation Codes for each Time_Slot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_Code|Count|\n",
      "+--------------+-----+\n",
      "|            21|34703|\n",
      "|            40|23628|\n",
      "|            14|14168|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code,  count(Violation_Code) as Count from binTab where Time_Bin=\"Slot_1\" \\\n",
    "            group by Violation_Code order by Count desc limit 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slot_1, the top 3 Violation_Code are `21`, `40`, `14`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_Code|Count|\n",
      "+--------------+-----+\n",
      "|            14|74114|\n",
      "|            40|60652|\n",
      "|            21|57897|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code,  count(Violation_Code) as Count from binTab where Time_Bin=\"Slot_2\" \\\n",
    "            group by Violation_Code order by Count desc limit 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slot_2, the top 3 Violation_Code are `14`, `40`, `21`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            21|598068|\n",
      "|            36|348165|\n",
      "|            38|176570|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code,  count(Violation_Code) as Count from binTab where Time_Bin=\"Slot_3\" \\\n",
    "            group by Violation_Code order by Count desc limit 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slot_3, the top 3 Violation_Code are `21`, `36`, `38`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            36|286284|\n",
      "|            38|240795|\n",
      "|            37|167044|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code,  count(Violation_Code) as Count from binTab where Time_Bin=\"Slot_4\" \\\n",
    "            group by Violation_Code order by Count desc limit 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slot_4, the top 3 Violation_Code are `36`, `38`, `37`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            38|102855|\n",
      "|            14| 75902|\n",
      "|            37| 70345|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code,  count(Violation_Code) as Count from binTab where Time_Bin=\"Slot_5\" \\\n",
    "            group by Violation_Code order by Count desc limit 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slot_5, the top 3 Violation_Code are `38`, `14`, `37`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slot_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Violation_Code|Count|\n",
      "+--------------+-----+\n",
      "|             7|26293|\n",
      "|            40|22337|\n",
      "|            14|21045|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code,  count(Violation_Code) as Count from binTab where Time_Bin=\"Slot_6\" \\\n",
    "            group by Violation_Code order by Count desc limit 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slot_6, the top 3 Violation_Code are `7`, `40`, `14`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+\n",
      "|Violation_Code| Count|\n",
      "+--------------+------+\n",
      "|            21|768060|\n",
      "|            36|662765|\n",
      "|            38|542078|\n",
      "+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code,  count(Violation_Code) as Count from binTab  \\\n",
    "            group by Violation_Code order by Count desc limit 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So Most Frequent Codes are 21,36, 38, Let's Find out the most frequent time_slot for each of these codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violation Code 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Time_Bin| Count|\n",
      "+--------+------+\n",
      "|  Slot_3|598068|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Time_Bin , count(Violation_Code) as Count from binTab \\\n",
    "            where Violation_Code = 21 group by Time_Bin Order by Count Desc Limit 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `Violation Code 21` is most frequent in `slot_3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voilation Code 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Time_Bin| Count|\n",
      "+--------+------+\n",
      "|  Slot_3|348165|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Time_Bin , count(Violation_Code) as Count from binTab \\\n",
    "            where Violation_Code = 36 group by Time_Bin Order by Count Desc Limit 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `Violation Code 36` is most frequent in `slot_3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voilation Code 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Time_Bin| Count|\n",
      "+--------+------+\n",
      "|  Slot_4|240795|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Time_Bin , count(Violation_Code) as Count from binTab \\\n",
    "            where Violation_Code = 38 group by Time_Bin Order by Count Desc Limit 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `Violation Code 38` is most frequent in `slot_4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Let’s try and find some seasonality in this data:\n",
    "\n",
    "First, divide the year into a certain number of seasons, and find the frequencies of tickets for each season. (Hint: Use Issue Date to segregate into seasons.)\n",
    "\n",
    "Then, find the three most common violations for each of these seasons.\n",
    "(Hint: You can use an approach similar to the one mentioned in the hint for question 4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Issued_Date|\n",
      "+-----------+\n",
      "| 2017-06-14|\n",
      "| 2017-06-13|\n",
      "| 2017-01-11|\n",
      "| 2017-02-04|\n",
      "| 2017-01-26|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Issued_Date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to divide data into American Seasons based on month of issued_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def season_slot(year):\n",
    "    year = str(year)\n",
    "    if year[5:7] in [\"03\" , \"04\", \"05\"]:\n",
    "        return \"Spring\"\n",
    "    elif year[5:7] in [\"06\" , \"07\", \"08\"]:\n",
    "        return \"Summer\"\n",
    "    elif year[5:7] in [\"09\" , \"10\", \"11\"]:\n",
    "        return \"Fall\"\n",
    "    elif year[5:7] in [\"12\" , \"01\", \"02\"]:\n",
    "        return \"Winter\"\n",
    "    else:\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_udf = udf( lambda Issued_Date : season_slot((Issued_Date)),  StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|Issued_Date|Season|\n",
      "+-----------+------+\n",
      "| 2017-06-14|Summer|\n",
      "| 2017-06-13|Summer|\n",
      "| 2017-01-11|Winter|\n",
      "| 2017-02-04|Winter|\n",
      "| 2017-01-26|Winter|\n",
      "+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('Season', season_udf((df['Issued_Date'])))\n",
    "\n",
    "df.select('Issued_Date', 'Season').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"SeasonTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequencies of tickets for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Season|  Count|\n",
      "+------+-------+\n",
      "|Spring|2873339|\n",
      "|Winter|1704663|\n",
      "|Summer| 852855|\n",
      "|  Fall|    979|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Season ,count(*) as Count from SeasonTable group BY season order by count(*) DESC' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yes, we can clearly see seasonality as most number of Violations are in  `Spring` followed by  `Winter ` ,` Summer` and `Fall`  respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The three most common violations for each of these seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------+\n",
      "|Season|Violation_Code| Count|\n",
      "+------+--------------+------+\n",
      "|Summer|            21|127347|\n",
      "|Summer|            36| 96663|\n",
      "|Summer|            38| 83518|\n",
      "+------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Season, Violation_Code ,count(*) as Count from SeasonTable  WHERE season = \"Summer\" \\\n",
    "group BY Violation_Code, Season ORDER by Count DESC LIMIT 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `Summer` top 3 Violation_Code are `21`, `36`, `38`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------+\n",
      "|Season|Violation_Code| Count|\n",
      "+------+--------------+------+\n",
      "|Spring|            21|402408|\n",
      "|Spring|            36|344834|\n",
      "|Spring|            38|271167|\n",
      "+------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Season, Violation_Code ,count(*) as Count from SeasonTable  WHERE season = \"Spring\" \\\n",
    "group BY Violation_Code, Season ORDER by Count DESC LIMIT 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `Spring` top 3 Violation_Code are `21`, `36`, `38`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+------+\n",
      "|Season|Violation_Code| count|\n",
      "+------+--------------+------+\n",
      "|Winter|            21|238177|\n",
      "|Winter|            36|221268|\n",
      "|Winter|            38|187385|\n",
      "+------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT Season, Violation_Code ,count(*) as count from SeasonTable  WHERE season = \"Winter\" \\\n",
    "group BY Violation_Code, Season ORDER by count DESC LIMIT 3' ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `Winter` top 3 Violation_Code are `21`, `36`, `38`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Violation_Code|Violation_Code_Count|\n",
      "+--------------+--------------------+\n",
      "|            21|              768085|\n",
      "|            36|              662765|\n",
      "|            38|              542079|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code, count(*) as Violation_Code_Count   FROM dfTable group by Violation_Code \\\n",
    "          ORDER by Violation_Code_Count Desc limit 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fines:\n",
    "* For code 21  fine = 65+45/2 = 55 dollars\n",
    "* For code 36 fine = 50+50/2 = 50 dollars\n",
    "* For code 38 fine = (65+35)/2 = 50 dollars \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calulating revenue generated by each fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------+\n",
      "|Violation_Code|Violation_Code_Count| Revenue|\n",
      "+--------------+--------------------+--------+\n",
      "|            21|              768085|42244675|\n",
      "|            36|              662765|33138250|\n",
      "|            38|              542079|27103950|\n",
      "+--------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code, count(*) as Violation_Code_Count , \\\n",
    "          CASE \\\n",
    "              WHEN Violation_Code=\"21\" THEN count(*)*55 \\\n",
    "              WHEN Violation_Code=\"36\" THEN count(*)*50 \\\n",
    "              WHen Violation_Code=\"38\" THEN count(*)*50 \\\n",
    "          END as Revenue \\\n",
    "          FROM dfTable group by Violation_Code \\\n",
    "          ORDER by Violation_Code_Count Desc limit 3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting violation code which results in Maximum revenue for Traffic Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------+\n",
      "|Violation_Code|Violation_Code_Count| Revenue|\n",
      "+--------------+--------------------+--------+\n",
      "|            21|              768085|42244675|\n",
      "+--------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT  Violation_Code, count(*) as Violation_Code_Count , \\\n",
    "          CASE \\\n",
    "              WHEN Violation_Code=\"21\" THEN count(*)*55 \\\n",
    "              WHEN Violation_Code=\"36\" THEN count(*)*50 \\\n",
    "              WHEN Violation_Code=\"38\" THEN count(*)*50 \\\n",
    "          END as Revenue \\\n",
    "          FROM dfTable where Violation_Code in (\"21\" , \"36\" , \"38\") \\\n",
    "          group by Violation_Code \\\n",
    "          ORDER by Revenue Desc limit 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Ordered by Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
